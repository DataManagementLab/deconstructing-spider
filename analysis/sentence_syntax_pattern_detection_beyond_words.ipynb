{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Syntax Pattern Detection on Sentence Level (beyond word boundaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from json import JSONDecodeError\n",
    "import stanza\n",
    "import json"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Setup WordNet & Stopword List"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "\n",
    "stopwords_en = set(stopwords.words('english'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Singular/plural handling"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import inflect\n",
    "\n",
    "inflector = inflect.engine()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Graph libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import igraph"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define placeholders"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class ChangedBy(Enum):\n",
    "    DB_TABLE = 1\n",
    "    DB_COLUMN = 2\n",
    "    DB_VAL = 3\n",
    "    NER = 4\n",
    "    NUM = 5\n",
    "    SYNONYM = 6\n",
    "    DB_PARTIAL_COLUMN = 7\n",
    "\n",
    "\n",
    "TABLE_NAME = '{TABLE}'\n",
    "COLUMN_NAME = '{COLUMN}'\n",
    "COLUMN_PARTIAL_NAME = '{COLUMN_PART}'\n",
    "DB_VALUE = '{VALUE}'\n",
    "NUMERICAL = '{NUMBER}'\n",
    "NAMED_ENTITY = '{NE}'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Specify Spider database"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "db_name = 'party_people'\n",
    "spider_directory = 'spider_data'\n",
    "db_directory = f'{spider_directory}/database/{db_name}'\n",
    "patterns_directory = f'{spider_directory}/patterns_no_synonyms'\n",
    "schema_file = f'{db_directory}/schema.sql'\n",
    "db_file = f'{db_directory}/{db_name}.sqlite'\n",
    "data_file = f'{spider_directory}/train_spider.json'\n",
    "tables_file = f'{spider_directory}/tables.json'\n",
    "export_trees_file = f'{spider_directory}/patterns.{db_name}.json'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Function: Load schema information from SQL file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def build_set_from_spider_element_list(entries):\n",
    "    \"\"\"\n",
    "    Build a set of db element entries\n",
    "    including transformation to lower text and singular/pluaral-inflection\n",
    "\n",
    "    :param entries: original entries\n",
    "    :type entries:\n",
    "    :return:\n",
    "    :rtype:\n",
    "    \"\"\"\n",
    "    entry_set = set()\n",
    "    for e in entries:\n",
    "        entry_set.add(e)\n",
    "        entry_set.add(e.lower())\n",
    "        plural_text = inflector.plural(e)\n",
    "        if plural_text and not plural_text.endswith(\"ss\"):\n",
    "            entry_set.add(plural_text)\n",
    "            entry_set.add(plural_text.lower())\n",
    "        singular_text = inflector.singular_noun(e)\n",
    "        if singular_text:\n",
    "            entry_set.add(singular_text)\n",
    "            entry_set.add(singular_text.lower())\n",
    "    return entry_set\n",
    "\n",
    "\n",
    "def load_schema(db_id, tables_file_path):\n",
    "    \"\"\"\n",
    "    Load schema information from tables.json\n",
    "\n",
    "    :param db_id: database\n",
    "    :type db_id: str\n",
    "    :param tables_file_path: path to tables.json\n",
    "    :type tables_file_path: str\n",
    "    :return: db schema information, set of table names, set of column names\n",
    "    :rtype: Dict[Str, Any], set[str], set[str]\n",
    "    \"\"\"\n",
    "    with open(tables_file_path, \"r\") as tables_json:\n",
    "        dbs = json.load(tables_json)\n",
    "        for db_schema in dbs:\n",
    "            if db_schema[\"db_id\"] == db_id:\n",
    "                column_names = build_set_from_spider_element_list(c for (_, c) in db_schema[\"column_names\"])\n",
    "\n",
    "                original_column_names = build_set_from_spider_element_list(c for (_, c) in db_schema[\"column_names_original\"])\n",
    "                column_names |= original_column_names\n",
    "\n",
    "                column_partial_names = set()\n",
    "                for column_name in column_names:\n",
    "                    column_partial_names.update(t for t in column_name.split() if not t in stopwords_en)\n",
    "\n",
    "                table_names = build_set_from_spider_element_list(db_schema[\"table_names\"])\n",
    "\n",
    "                table_names_list = list(table_names)\n",
    "                table_names_list.sort(key=lambda x: len(x), reverse=True)\n",
    "\n",
    "                column_names_list = list(column_names)\n",
    "                column_names_list.sort(key=lambda x: len(x), reverse=True)\n",
    "                \n",
    "                column_partial_names_list = list(column_partial_names)\n",
    "                column_partial_names_list.sort(key=lambda x: len(x), reverse=True)\n",
    "\n",
    "                return db_schema, table_names_list, column_names_list, column_partial_names_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Function: Load database values from file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "problem_dbs = set()\n",
    "\n",
    "def load_db_values(schema_file_path):\n",
    "    \"\"\"\n",
    "    Load all database values from file that are not stopwords\n",
    "\n",
    "    :param schema_file_path: data base schema definition file\n",
    "    :type schema_file_path: str\n",
    "    :return: All values of the selected database\n",
    "    :rtype: Set[Any]\n",
    "    \"\"\"\n",
    "    db_values = set()\n",
    "    try:\n",
    "        with open(schema_file_path, 'r') as schema_file:\n",
    "            for line in schema_file.readlines():\n",
    "                if line.startswith(\"INSERT INTO\"):\n",
    "                    # Get values part of insert string but without the surrounding blanks, brackets and the semicolon\n",
    "                    values_raw = line.split(\"VALUES\")[-1][2:-3]\n",
    "                    try:\n",
    "                        db_values.update(str(v) for v in json.loads(f'[{values_raw}]') if not v in stopwords_en)\n",
    "                    except JSONDecodeError:\n",
    "                        problem_dbs.add(schema_file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"{schema_file_path} not found\")\n",
    "        problem_dbs.add(schema_file_path)\n",
    "    return db_values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load DB Schema & values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "db_schema, db_tables, db_colums, db_partial_columns = load_schema(db_name, tables_file)\n",
    "print(db_schema)\n",
    "print(db_tables)\n",
    "print(db_colums)\n",
    "db_values = load_db_values(schema_file)\n",
    "print(db_values)\n",
    "\n",
    "db_info = {\n",
    "    \"schema\": db_schema,\n",
    "    \"tables\": db_tables,\n",
    "    \"columns\": db_colums,\n",
    "    \"partial_columns\": db_partial_columns,\n",
    "    \"values\": db_values\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Function: Load NL-SQL pairs for given Spider database"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_train_data_for_db(db, db_data_file):\n",
    "    \"\"\"\n",
    "    Load all data (NL-SQL) entries for the selected database\n",
    "\n",
    "    :param db: database\n",
    "    :type db: str\n",
    "    :param db_data_file: path of file containing NL-SQL entries\n",
    "    :type db_data_file: str\n",
    "    :return: NL-SQL-Entries\n",
    "    :rtype: List[Dict]\n",
    "    \"\"\"\n",
    "    with open(db_data_file, \"r\") as train_json:\n",
    "        return [entry for entry in json.load(train_json) if entry[\"db_id\"] == db]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load NL-SQL pairs for DB"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = load_train_data_for_db(db_name, data_file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Setup Stanza"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stanza.download('en') # download English model\n",
    "nlp = stanza.Pipeline('en') # initialize English neural pipeline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Functions: Get start or end index of a given word in stanza parse format"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import re\n",
    "start_matcher = re.compile(\"start_char=(\\d+)\")\n",
    "end_matcher = re.compile(\"end_char=(\\d+)\")\n",
    "\n",
    "def get_word_start(misc_string):\n",
    "    \"\"\"\n",
    "    Get start of a word from a stanza word misc attribute string\n",
    "\n",
    "    :param misc_string: stanza misc attribute string\n",
    "    :type misc_string: str\n",
    "    :return: start character of the given parsed word\n",
    "    :rtype: int\n",
    "    \"\"\"\n",
    "    return int(start_matcher.search(misc_string).group(1))\n",
    "\n",
    "\n",
    "def get_word_end(misc_string):\n",
    "    \"\"\"\n",
    "    Get end of a word from a stanza word misc attribute string\n",
    "\n",
    "    :param misc_string: stanza misc attribute string\n",
    "    :type misc_string: str\n",
    "    :return: end character of the given parsed word\n",
    "    :rtype: int\n",
    "    \"\"\"\n",
    "    return int(end_matcher.search(misc_string).group(1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data structures for parse tree container and processed token list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ParseTree:\n",
    "    def __init__(self, orginal_nl_query, original_sql_query, tokens, raw_parsed, id=-1):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "\n",
    "        :param orginal_nl_query: original NL query\n",
    "        :type orginal_nl_query: str\n",
    "        :param original_sql_query: original SQL query\n",
    "        :type original_sql_query: str\n",
    "        :param tokens: list of tokens\n",
    "        :type tokens: List[Token]\n",
    "        :param raw_parsed: raw stanza sentence parse\n",
    "        :type raw_parsed: stanza.Sentence\n",
    "        \"\"\"\n",
    "        self.orginal_nl_query = orginal_nl_query\n",
    "        self.original_sql_query = original_sql_query\n",
    "        self.tokens = tokens\n",
    "        self.ents = raw_parsed.ents\n",
    "        self.raw_parsed = raw_parsed\n",
    "        self.id = id\n",
    "        # self.log = []\n",
    "        # self.graph = igraph.Graph()\n",
    "        # self.graph.add_vertices(len(self.tokens))\n",
    "\n",
    "\n",
    "    def to_tree(self):\n",
    "\n",
    "        tree_rec = defaultdict(list)\n",
    "        for token in self.tokens:\n",
    "            tree_rec[token.head()].append(token)\n",
    "\n",
    "        def _to_tree_rec(head_id):\n",
    "            if head_id in tree_rec:\n",
    "                tokens = tree_rec[head_id]\n",
    "                if len(tokens) > 0:\n",
    "                    return f\"({' '.join(str(t))}\"\n",
    "            pass\n",
    "        return \" \".join(str(t) for t in tree_rec[0]), tree_rec\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{'|'.join(str(t) for t in self.tokens)}\"\n",
    "\n",
    "\n",
    "class Token:\n",
    "    def __init__(self, text, parsed_tokens, changed=True, changed_by='', ent=None):\n",
    "        self.text = text\n",
    "        self.original_text = \" \".join(p.text for p in parsed_tokens)\n",
    "        self.parsed_tokens = parsed_tokens\n",
    "        self.changed = changed\n",
    "        self.changed_by = changed_by\n",
    "        self.is_compound = len(self.parsed_tokens) > 1\n",
    "        self.ent = ent\n",
    "\n",
    "    def ids(self):\n",
    "        \"\"\"\n",
    "        Get all ids of contained parsed words\n",
    "\n",
    "        :return: list of all ids\n",
    "        :rtype: List[int]\n",
    "        \"\"\"\n",
    "        return [w.id for w in self.parsed_tokens]\n",
    "\n",
    "    def head(self):\n",
    "        \"\"\"\n",
    "        Get head id (highest in list)\n",
    "\n",
    "        :return: head id\n",
    "        :rtype: int\n",
    "        \"\"\"\n",
    "        return min(w.head for w in self.parsed_tokens)\n",
    "\n",
    "    def __str__(self):\n",
    "        if self.changed:\n",
    "            return f\"{self.text} [{self.original_text}]\"\n",
    "        return self.text\n",
    "\n",
    "\n",
    "def get_words_in_range(words_by_start, current_char, next_char):\n",
    "    \"\"\"\n",
    "    Get words in given range\n",
    "\n",
    "    :param words_by_start: dictionary of words (by start)\n",
    "    :type words_by_start: OrderedDict[int, stanza.Word]\n",
    "    :param current_char: start\n",
    "    :type current_char: int\n",
    "    :param next_char: end\n",
    "    :type next_char: int\n",
    "    :return: list of all words in given range\n",
    "    :rtype: List[stanza.Word]\n",
    "    \"\"\"\n",
    "    return [w for (k, w) in words_by_start.items() if k >= current_char and (k + len(w.text)) <= next_char]\n",
    "\n",
    "\n",
    "def create_unchanged(current_char, question_remainder, words_by_start, ents_by_start, db_data):\n",
    "    \"\"\"\n",
    "    Create token for unchanged word\n",
    "\n",
    "    :param current_char:\n",
    "    :type current_char:\n",
    "    :param question_remainder:\n",
    "    :type question_remainder:\n",
    "    :param words_by_start:\n",
    "    :type words_by_start:\n",
    "    :param ents_by_start:\n",
    "    :type ents_by_start:\n",
    "    :param db_data:\n",
    "    :type db_data:\n",
    "    :return:\n",
    "    :rtype:\n",
    "    \"\"\"\n",
    "    current_word = words_by_start[current_char]\n",
    "    return Token(current_word.text, [current_word], changed=False), current_char + len(current_word.text)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Functions: Replace table and partial column names as well as db values in the parse tree with a placeholder (and add provenance)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def replace_db_element(current_char, question_remainder, words_by_start, db_elements, placeholder, changed_by):\n",
    "    \"\"\"\n",
    "    Replace db elements with placeholder\n",
    "\n",
    "    :param current_char: current position in question string\n",
    "    :type current_char: int\n",
    "    :param question_remainder: unprocessed question string\n",
    "    :type question_remainder: str\n",
    "    :param words_by_start: dictionary of words by their start character\n",
    "    :type words_by_start: Dict[int, stanza.Word]\n",
    "    :param db_elements: elements of the database to search for\n",
    "    :type db_elements: List[str]\n",
    "    :param placeholder: placeholder to replace with\n",
    "    :type placeholder: str\n",
    "    :param changed_by: identifier for replacing pipeline step\n",
    "    :type changed_by: Enum[ChangedBy]\n",
    "    :return: newly created token (if any) and next character to start processing with\n",
    "    :rtype: Token, int\n",
    "    \"\"\"\n",
    "    for element in db_elements:\n",
    "        qr_lower = question_remainder.lower()\n",
    "        if question_remainder.startswith(element) or qr_lower.startswith(element):\n",
    "            next_char = current_char +  len(element)\n",
    "            affected_words = get_words_in_range(words_by_start, current_char, next_char)\n",
    "\n",
    "            if len(affected_words) > 0 and get_word_end(affected_words[-1].misc) == next_char:\n",
    "                return Token(placeholder, affected_words, changed_by=changed_by), next_char\n",
    "\n",
    "    return None, current_char\n",
    "\n",
    "\n",
    "def replace_column_names(current_char, question_remainder, words_by_start, ents_by_start, db_info):\n",
    "    \"\"\"\n",
    "    Replace column names\n",
    "\n",
    "    :param current_char: current position in question string\n",
    "    :type current_char: int\n",
    "    :param question_remainder: unprocessed question string\n",
    "    :type question_remainder: str\n",
    "    :param words_by_start: dictionary of words by their start character\n",
    "    :type words_by_start: Dict[int, stanza.Word]\n",
    "    :param ents_by_start: dictionary of entities by their start character\n",
    "    :type ents_by_start: Dict[int, stanza.Entity]\n",
    "    :param db_info: dictionary containing database information\n",
    "    :type db_info: Dict[str, Any]\n",
    "    :return: newly created token (if any) and next character to start processing with\n",
    "    :rtype: Token, int\n",
    "    \"\"\"\n",
    "    return replace_db_element(current_char, question_remainder, words_by_start, db_info[\"columns\"], COLUMN_NAME, ChangedBy.DB_COLUMN)\n",
    "\n",
    "\n",
    "def replace_table_names(current_char, question_remainder, words_by_start, ents_by_start, db_info):\n",
    "    \"\"\"\n",
    "    Replace table names\n",
    "\n",
    "    :param current_char: current position in question string\n",
    "    :type current_char: int\n",
    "    :param question_remainder: unprocessed question string\n",
    "    :type question_remainder: str\n",
    "    :param words_by_start: dictionary of words by their start character\n",
    "    :type words_by_start: Dict[int, stanza.Word]\n",
    "    :param ents_by_start: dictionary of entities by their start character\n",
    "    :type ents_by_start: Dict[int, stanza.Entity]\n",
    "    :param db_info: dictionary containing database information\n",
    "    :type db_info: Dict[str, Any]\n",
    "    :return: newly created token (if any) and next character to start processing with\n",
    "    :rtype: Token, int\n",
    "    \"\"\"\n",
    "    return replace_db_element(current_char, question_remainder, words_by_start, db_info[\"tables\"], TABLE_NAME, ChangedBy.DB_TABLE)\n",
    "\n",
    "\n",
    "def replace_column_partial_names(current_char, question_remainder, words_by_start, ents_by_start, db_info):\n",
    "    \"\"\"\n",
    "    Replace partial column names\n",
    "\n",
    "    :param current_char: current position in question string\n",
    "    :type current_char: int\n",
    "    :param question_remainder: unprocessed question string\n",
    "    :type question_remainder: str\n",
    "    :param words_by_start: dictionary of words by their start character\n",
    "    :type words_by_start: Dict[int, stanza.Word]\n",
    "    :param ents_by_start: dictionary of entities by their start character\n",
    "    :type ents_by_start: Dict[int, stanza.Entity]\n",
    "    :param db_info: dictionary containing database information\n",
    "    :type db_info: Dict[str, Any]\n",
    "    :return: newly created token (if any) and next character to start processing with\n",
    "    :rtype: Token, int\n",
    "    \"\"\"\n",
    "    return replace_db_element(current_char, question_remainder, words_by_start, db_info[\"partial_columns\"], COLUMN_PARTIAL_NAME, ChangedBy.DB_PARTIAL_COLUMN)\n",
    "\n",
    "\n",
    "def replace_db_values(current_char, question_remainder, words_by_start, ents_by_start, db_info):\n",
    "    \"\"\"\n",
    "    Replace partial column names\n",
    "\n",
    "    :param current_char: current position in question string\n",
    "    :type current_char: int\n",
    "    :param question_remainder: unprocessed question string\n",
    "    :type question_remainder: str\n",
    "    :param words_by_start: dictionary of words by their start character\n",
    "    :type words_by_start: Dict[int, stanza.Word]\n",
    "    :param ents_by_start: dictionary of entities by their start character\n",
    "    :type ents_by_start: Dict[int, stanza.Entity]\n",
    "    :param db_info: dictionary containing database information\n",
    "    :type db_info: Dict[str, Any]\n",
    "    :return: newly created token (if any) and next character to start processing with\n",
    "    :rtype: Token, int\n",
    "    \"\"\"\n",
    "    return replace_db_element(current_char, question_remainder, words_by_start, db_info[\"values\"], DB_VALUE, ChangedBy.DB_VAL)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Function: Replace all Named Entities and numerical values in the parse tree with placeholders (and add provenance)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def replace_named_entities_and_numbers(current_char, question_remainder, words_by_start, ents_by_start, db_info):\n",
    "    \"\"\"\n",
    "    Replace named entitites and numbers\n",
    "\n",
    "    :param current_char: current position in question string\n",
    "    :type current_char: int\n",
    "    :param question_remainder: unprocessed question string\n",
    "    :type question_remainder: str\n",
    "    :param words_by_start: dictionary of words by their start character\n",
    "    :type words_by_start: Dict[int, stanza.Word]\n",
    "    :param ents_by_start: dictionary of entities by their start character\n",
    "    :type ents_by_start: Dict[int, stanza.Entity]\n",
    "    :param db_info: dictionary containing database information\n",
    "    :type db_info: Dict[str, Any]\n",
    "    :return: newly created token (if any) and next character to start processing with\n",
    "    :rtype: Token, int\n",
    "    \"\"\"\n",
    "    current_word = words_by_start[current_char]\n",
    "\n",
    "    if current_char in ents_by_start:\n",
    "        ent = ents_by_start[current_char]\n",
    "        next_char = current_char + len(ent.text)\n",
    "        if ent.type != \"CARDINAL\":\n",
    "            return Token(NAMED_ENTITY, get_words_in_range(words_by_start, current_char, next_char), ent=ent, changed_by=ChangedBy.NER), next_char\n",
    "        else:\n",
    "            return Token(NUMERICAL, get_words_in_range(words_by_start, current_char, next_char), ent=ent, changed_by=ChangedBy.NUM), next_char\n",
    "    elif current_word.upos == \"NUM\":\n",
    "        next_char = current_char + len(current_word.text)\n",
    "        return Token(NUMERICAL, get_words_in_range(words_by_start, current_char, next_char), changed_by=ChangedBy.NUM), next_char\n",
    "\n",
    "    return None, current_char"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Retrieve synset(s) for token"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Map between stanza and wordnet POS tag format\n",
    "POS_TO_WN_POS = {\"NOUN\": wordnet.NOUN, \"ADJ\": wordnet.ADJ, \"VERB\": wordnet.VERB, \"ADV\": wordnet.ADV}\n",
    "\n",
    "def retrieve_synset(word, pos_tag):\n",
    "    \"\"\"\n",
    "    Get synonym for a given word using WordNet\n",
    "\n",
    "    :param word: word to get synonym for\n",
    "    :type word: str\n",
    "    :param pos_tag: POS tag of given word\n",
    "    :type pos_tag: str\n",
    "    :return: synonym\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    if pos_tag in POS_TO_WN_POS:\n",
    "        containing_synsets = [synset.name().split('.')[0] for synset in wordnet.synsets(word, pos=POS_TO_WN_POS[pos_tag])]\n",
    "        if len(containing_synsets) > 0:\n",
    "            fd = nltk.FreqDist(containing_synsets)\n",
    "            return fd.max()\n",
    "    return word"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Function: Replace non-placeholder non-stopword tokens in parse tree with corresponding Synset name"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def replace_synonyms(current_char, question_remainder, words_by_start, ents_by_start, db_info):\n",
    "    \"\"\"\n",
    "    Replace words with synonyms (synset-based)\n",
    "\n",
    "    :param current_char: current position in question string\n",
    "    :type current_char: int\n",
    "    :param question_remainder: unprocessed question string\n",
    "    :type question_remainder: str\n",
    "    :param words_by_start: dictionary of words by their start character\n",
    "    :type words_by_start: Dict[int, stanza.Word]\n",
    "    :param ents_by_start: dictionary of entities by their start character\n",
    "    :type ents_by_start: Dict[int, stanza.Entity]\n",
    "    :param db_info: dictionary containing database information\n",
    "    :type db_info: Dict[str, Any]\n",
    "    :return: newly created token (if any) and next character to start processing with\n",
    "    :rtype: Token, int\n",
    "    \"\"\"\n",
    "    current_word = words_by_start[current_char]\n",
    "\n",
    "    if not current_word.text in stopwords_en or current_word.text.lower() in stopwords_en:\n",
    "        synonym = retrieve_synset(current_word.text, current_word.upos)\n",
    "        if synonym != current_word.text:\n",
    "            return Token(synonym, [current_word], changed_by=ChangedBy.SYNONYM), current_char + len(current_word.text)\n",
    "\n",
    "    return None, current_char"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "REPLACEMENT_PIPELINE = [\n",
    "    replace_column_names,\n",
    "    replace_table_names,\n",
    "    replace_db_values,\n",
    "    replace_column_partial_names,\n",
    "    replace_named_entities_and_numbers,\n",
    "    # replace_synonyms,\n",
    "    create_unchanged\n",
    "]\n",
    "\n",
    "\n",
    "def parse_and_replace(nl_sql, db_info, id=-1):\n",
    "    \"\"\"\n",
    "    Generated parsed form of given nl-sql entry\n",
    "\n",
    "    :param nl_sql: data entry containing a NL query\n",
    "    :type nl_sql: Dict[Str, Any]\n",
    "    :return: parse tree\n",
    "    :rtype: ParseTree\n",
    "    \"\"\"\n",
    "    # Parse and return parsed form of first sentence (there should always be only one)\n",
    "    question = nl_sql[\"question\"]\n",
    "    parsed_all = nlp(question)\n",
    "    if len(parsed_all.sentences) > 1:\n",
    "        raise ValueError\n",
    "    parsed = parsed_all.sentences[0]\n",
    "\n",
    "    ents_by_start = {e.start_char: e for e in parsed.ents}\n",
    "    words_by_start = {get_word_start(w.misc): w for w in parsed.words}\n",
    "\n",
    "    current_char = 0\n",
    "    tokens = []\n",
    "\n",
    "    while current_char < len(nl_sql[\"question\"]):\n",
    "        question_remainder = question[current_char:]\n",
    "\n",
    "        if question_remainder.startswith(\" \"):\n",
    "            current_char += 1\n",
    "            continue\n",
    "\n",
    "        if current_char in words_by_start:\n",
    "            current_word = words_by_start[current_char]\n",
    "            if current_word.upos == \"PUNCT\":\n",
    "                current_char += len(current_word.text)\n",
    "                continue\n",
    "        else:\n",
    "            print(words_by_start)\n",
    "            print(question_remainder)\n",
    "\n",
    "        for pipeline_step in REPLACEMENT_PIPELINE:\n",
    "            token, next_char = pipeline_step(current_char, question_remainder, words_by_start, ents_by_start, db_info)\n",
    "            if token is not  None:\n",
    "                tokens.append(token)\n",
    "                current_char = next_char\n",
    "                break\n",
    "\n",
    "    return ParseTree(nl_sql[\"question\"], nl_sql[\"query\"], tokens, parsed, id=id)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Apply parse and replace for some sample sentences"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(db_info[\"columns\"])\n",
    "\n",
    "sample_tree = parse_and_replace(data[7], db_info)\n",
    "print(sample_tree)\n",
    "print(*[f'id: {word.id}\\tword: {word.text}\\thead id: {word.head}\\thead: {sample_tree.raw_parsed.words[word.head-1].text if word.head > 0 else \"root\"}\\tdeprel: {word.deprel}' for word in sample_tree.raw_parsed.words], sep='\\n')\n",
    "print(sample_tree.to_tree())\n",
    "\n",
    "sample_tree = parse_and_replace(data[27], db_info)\n",
    "print(sample_tree)\n",
    "sample_tree = parse_and_replace(data[42], db_info)\n",
    "print(sample_tree)\n",
    "sample_tree = parse_and_replace(data[15], db_info)\n",
    "print(sample_tree)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generate trees for every NL-SQL pair in this DB"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "db_parse_trees = []\n",
    "for nl_sql_pair in data:\n",
    "    db_parse_trees.append(parse_and_replace(nl_sql_pair, db_info))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Store string transcoding results in a file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "patterns = [\n",
    "    {\n",
    "        \"original_nl\": pt.orginal_nl_query,\n",
    "        \"generalized_nl\": str(pt),\n",
    "        \"generalized_tokens\": [t.text for t in pt.tokens],\n",
    "        \"original_sql\": pt.original_sql_query\n",
    "    } for pt in db_parse_trees]\n",
    "\n",
    "with open(export_trees_file, \"w\") as pattern_file:\n",
    "    json.dump(patterns, pattern_file, indent=4)\n",
    "    print(f\"Generalized {len(patterns)} queries and stored them in '{export_trees_file}'\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Parse all the databases!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def export(last_db_name, db_parse_trees):\n",
    "    patterns = [\n",
    "    {\n",
    "        \"original_nl\": pt.orginal_nl_query,\n",
    "        \"generalized_nl\": str(pt),\n",
    "        \"generalized_tokens\": [t.text for t in pt.tokens],\n",
    "        \"original_sql\": pt.original_sql_query,\n",
    "        \"id\": pt.id\n",
    "    } for pt in db_parse_trees]\n",
    "\n",
    "    export_trees_file = f'{patterns_directory}/{last_db_name}.json'\n",
    "\n",
    "    with open(export_trees_file, \"w\") as pattern_file:\n",
    "        json.dump(patterns, pattern_file, indent=4)\n",
    "        print(f\"Generalized {len(patterns)} queries and stored them in '{export_trees_file}'\")\n",
    "\n",
    "\n",
    "def parse_all_and_store_patterns(nl_sql_file):\n",
    "    last_db_name = \"\"\n",
    "    db_parse_trees = []\n",
    "    error_counter = 0\n",
    "    success_counter = 0\n",
    "\n",
    "    with open(nl_sql_file, \"r\") as train_json:\n",
    "        for i, entry in enumerate(json.load(train_json)):\n",
    "            if entry[\"db_id\"] != last_db_name:\n",
    "                if last_db_name != \"\":\n",
    "                    # Store previous patterns if necessary\n",
    "                    export(last_db_name, db_parse_trees)\n",
    "\n",
    "                last_db_name = entry[\"db_id\"]\n",
    "                print(last_db_name)\n",
    "\n",
    "                # Reset/load new db info\n",
    "                db_directory = f'{spider_directory}/database/{last_db_name}'\n",
    "                schema_file = f'{db_directory}/schema.sql'\n",
    "\n",
    "                db_schema, db_tables, db_colums, db_partial_columns = load_schema(last_db_name, tables_file)\n",
    "                db_values = load_db_values(schema_file)\n",
    "\n",
    "                db_info = {\n",
    "                    \"schema\": db_schema,\n",
    "                    \"tables\": db_tables,\n",
    "                    \"columns\": db_colums,\n",
    "                    \"partial_columns\": db_partial_columns,\n",
    "                    \"values\": db_values\n",
    "                }\n",
    "\n",
    "                db_parse_trees = []\n",
    "\n",
    "            # Parse and replace the current nnl_sql_pair\n",
    "            # print(f'{i}: {entry[\"question\"]}')\n",
    "            try:\n",
    "                db_parse_trees.append(parse_and_replace(entry, db_info))\n",
    "                success_counter += 1\n",
    "            except ValueError:\n",
    "                print(\"Multiple Sentences, did not export\")\n",
    "                error_counter += 1\n",
    "\n",
    "    export(last_db_name, db_parse_trees)\n",
    "\n",
    "    print(f\"Finished export. Exported {success_counter} entries, an eror occurred for {error_counter} entries.\")\n",
    "\n",
    "\n",
    "parse_all_and_store_patterns(data_file)\n",
    "print(\"Problem DBs:\", problem_dbs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}